
---
csl: canadian-journal-of-fisheries-and-aquatic-sciences.csl
output:
  bookdown::pdf_document2:
    toc: no
    number_sections: no
    fig_caption: yes
    keep_tex: yes
  bibliography: IBMWG.bib
  
  word_document: default
  html_document: default
  pdf_document:
    toc: no
header-includes:
- \usepackage{setspace}
- \doublespacing
- \usepackage{lineno}
- \linenumbers
- \usepackage[belowskip=0pt,aboveskip=0pt]{caption}
 \usepackage{array}
 \usepackage{caption}
 \usepackage{graphicx}
 \usepackage{siunitx}
 \usepackage{colortbl}
 \usepackage{multirow}
 \usepackage{hhline}
 \usepackage{calc}
 \usepackage{tabularx}
 \usepackage{tabulary}
 \usepackage{threeparttable}
 \usepackage{wrapfig}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
if(!require(kableExtra)) {  
  install.packages("kableExtra")
  require(kableExtra)}



#read in results tables
YT_results<-read.csv("Yellowtail_results.csv")
Cod_results<-read.csv("Cod_results.csv")
Had_results<-read.csv("Haddock_results.csv")
```


\newpage

# Estimating Population Trends with Stratified Random Sampling Under the Pressures of Climate Change

  
Benjamin A. Levy^1^, Christopher M. Legault^2^, Timothy J. Miller^2^, Elizabeth N. Brooks^2^



^1^Ben's Institution, USA  
^2^National Marine Fisheries Service, Northeast Fisheries Science Center, Woods Hole, MA, USA  
  

Corresponding author: Ben Levy (benjamin.levy@noaa.gov)

Competing interests: The authors declare there are no competing interests.

<!-- Table1 <- YT_results -->

<!-- write.table( -->
<!--    format(f, digits=2), "", -->
<!--    sep="|", row.names=F, col.names=F, quote=F, eol="|\n") -->

\newpage

## Abstract

An Abstract

## Keywords

keyword 1, keyword 2

\newpage

\section{Introduction}


much of below is from https://apps-nefsc.fisheries.noaa.gov/nefsc/ecosystem-ecology/
or
https://www.fisheries.noaa.gov/data-tools/fisheries-economics-united-states-data-and-visualizations


- The eastern continental shelf is ecologically diverse and economically important


The Northeast United States continental shelf spans from the Outer Banks of North Carolina to the Gulf of Maine. The region covers over 250,000 km$^2$ of ocean, extending over 200 km from shore in the largest areas in New England to just 30 km off shore in the southern regions. This ecologically diverse region contains approximately 18,000 vertebrate marine species. Commercial fisheries have been an important part of local economies for centuries. In 2019, New England fisheries produced $22 billion in sales, which sustained over 200,000 jobs. Maintaining a healthy ecosystem is therefore vital to sustained ecological health and economic prosperity of the region. [@nefmc20]


- Bottom trawl survey is important for monitoring population trends

Fish stocks in this highly productive and economically important region are managed by the National Oceanic and Atmospheric Administration’s (NOAA) Northeast Fisheries Science Center (NEFSC) in Woods Hole, Massachusetts. Federal biologists assess the health and abundance of each commercial fish stock using fishery-independent bottom trawl survey data that has been collected by NOAA throughout the region since 1963 (cite survey paper). The survey uses a stratified random design where bottom trawl sampling takes place in predefined strata along the eastern continental shelf. The survey has created a rich time series data set with many uses including species-specific habitat identification, analysis of how environmental conditions influence species abundance, and estimating yearly species abundance trends to help inform stock assessments and ultimately quota limits **just listed a few uses of survey change/others?**.  

The survey takes place twice each year- once in the spring and again in the fall. Since most spatial analyses and  projections of future distributions typically assume a constant survey catchability and/or availability over time, NOAA's survey design includes sampling during approximately the same 2-3 week time period in each season.  



- Climate change is happening
 Due to a combination of climate change and shifts in circulation, the Northeast United States continental shelf has experienced rapid warming in recent decades, resulting in a shift in spatial distributions of many species. Since stock assessment models rely on accurate descriptions of population dynamics and contemporary patterns of spatial abundance, there is concern that rapid undocumented changes in spatial distributions of species will bias future stock assessments. The implication of this is that the bottom trawl survey is actually sampling the population during a different life cycle stage than was originally assumed, which can lead to biased stock assessments. We are therefore interested in analyzing the impact of climate change on the accuracy of future stock assessment models as measured by NOAA’s ongoing bottom-trawl survey along the East coast.

**use more info from initial proposal**

- Fish are changing spatial distribution and have altered life stages (?) because of climate change
NYE paper

- Population indexing methods may be becoming biased as a result

- Briefly describe our study to test this

To test the ability of the bottom trawl survey to track population trends under shifting environmental conditions, we construct spatial models for fish where movement depend on temperature preferences. We can then consider the impact of climate change by simulating scenarios with repeating temperature patterns and those where temperature increases on average over time. In both cases we analyze the ability of stratified random sampling to track population trends.  


\section{Methods}

- Describe simulation study

We construct spatial models for Yellowtail Flounder, Atlantic Cod, and Haddock on George's Bank, where movement of each species combine static species-specific habitat preferences with temperature preferences. Model dynamics are driven by a time series of temperature gradients that were estimated from data to create simulated data sets for each population where the true biomass is known. Using temperature gradients that repeat each year creates data sets with predictable, repeating spatial patterns, whereas using a temperature gradient that increases on average throughout the simulation leads to spatial distributions that shift over time. We conducting stratified random sampling on our simulation output to mimic the bottom trawl survey and compare the ability of contemporary indexing methods to track population trends. 

\subsection{Population Model Formulation}


-- Used MixFishSim. Describe edits made to package

We use the R package *MixFishSim* (MFS) to model our populations [@dolder2020highly]. MFS is a discrete spatiotemporal simulation tool where users can model multiple species under varying environmental conditions. The package uses a delay-difference population model with discrete processes for growth, death, and recruitment of the population. We formulate the following inputs for the MFS package to address our research question.


<font size="5">*Study Area*</font>

We obtained a shapefile for the 15 strata that comprise George's Bank to use as our modeling environment. We discritized the region into a raster with 88 rows and 144 columns. Haddock inhabit all 15 strata in the domain, Cod inhabit 13 strata, and Yellowtail exist in 9 strata. Figure \@ref(fig:strata-plot) shows the regions used in our models.

```{r strata-plot, echo=FALSE,out.width="95%",fig.show='hold', fig.align='center', fig.cap="Strata inhabited by each species in our population models."}
knitr::include_graphics(c("Images/Strata.png"))
```

<font size="5">*Population Dynamics and Recruitment*</font>

The time step for our models is one week. MFS uses a modified two-stage Deriso-Schnute delay difference equation that models the biomass in each cell in our study area [@dolder2020highly]. Individual terms in the formulation account for growth of mature adults, natural and fishing mortality, and the addition of new recruits. We chose to represent recruitment in the model using a Beverton-Holt formulation **cite**. Recruitment is a function of the adult biomass that existed in the previous year and is added to the population incrementally throughout each species' predefined spawning period. Parameter inputs were either obtained from the literature or chosen to produce desired model dynamics. A full list of parameters used in our model can be seen below in Tables \@ref(paramsALL) and \@ref(tab:paramsSCENARIOS). 



<font size="5">*Movement*</font>

The package was designed to generate theoretical habitat preferences using Gaussian Random Fields that combine with hypothetical temperature gradients to drive the probability of movement from cell $I$ to cell $J$ using the formulation

\begin{align}
Pr(C_{wk+1}=J|C_{wk}=I) = \frac{e^{-\lambda \cdot d_{I,J}}\cdot(Hab^2_{J,s} \cdot Tol_{J,s,wk})}{\sum^C_{c=1}e^{-\lambda \cdot d} \cdot (Hab^2_{c,s} \cdot Tol_{c,s,wk})},
\label{moveP}
\end{align}


 where 
 

   $e^{-\lambda \cdot d_{I,J}}$ accounts for distance between cells $I$ and $J$,
     
  $Hab^2_{J,s}$ is the static habitat value for species $s$ in cell $J$, and
    
   $Tol_{c,s,wk}$ is the value from normally distributed temperature tolerance for species $s$ in cell $c$ in week $wk$.


Since we are modeling real species on the northeast continental shelf, we formulate the habitat and temperature components as follows.

<font size="5">*Habitat Input*</font>

Species-specific habitat preferences were derived using the *lrren* tool from the R package *envi* **cite** to create a niche model for each species. The *lrren* tool estimates an ecological niche using the relative risk function by relating presence/absence data to two covariate predictors. We used bottom trawl point data from 2009-2021 as our presence/absence input by using a value of 0 for any tow that failed to catch the given species and weighting a successful catch by the biomass of the given tow **cite trawl data?**. Depth and mean sediment size were used as our covariate predictors **cite**. Since the values in $Hab^2_{J,s}$ are required to be between 0 and 1, we transform the spatial estimates from *lrren* to fall between these bounds. See Figure ??? for a visual representation of this process being applied to Cod. Figure \@ref(fig:hab-plot) depicts habitat preferences $Hab^2_{J,s}$ for each species.

```{r hab-plot, echo=FALSE,out.width="95%",fig.show='hold', fig.align='center', fig.cap="Static habitat preferences for each species in our population models (Yellotwtail, Cod, Haddock)."}
knitr::include_graphics(c("Images/Habitat_3species.png"))
```

<font size="5">*Temperature Input*</font>

Each species is assumed to have normally distributed temperature preferences ($N(\mu,\sigma)$). We assume Yellowtail Flounder's preferences are $N(8.75,4.25)$, while Haddock and Cod have preferences $N(9,4)$. We chose these values by combining information in the literature with temperatures recorded in the bottom trawl survey. Weekly estimated temperature data for the region for 2012 was obtained from FVCOM **cite**. We chose 2012 because the data displayed an average temperature pattern that consistently oscillated between maximum and minimum temperature values. This data was also transform to create an oscillating pattern that increases 5 degrees Celsius on average over the duration of the simulation.  **show images of temperature and/or and/or temp videos??? and/or average temperature oscillations?**


--Describe difference between increasing and constant temperature scenarios (images?)

In equation (1), $Hab^2_{J,s}$ is constant for the duration of the simulation, while $Tol_{c,s,wk}$ changes each week. Using a temperature gradient that repeats each year produces the same spatial preferences in a given week each year, which results in consistent spatial biomass patterns. Scenarios where the temperature increases over time creates spatial preferences that evolve as the water warms, which creates spatial biomass patterns that shift in a given week over the duration of the simulation.
 


-- Describe each scenario that is considered

We consider 20 year simulations under three population parameter scenarios for each of our three species- a scenario where parameters result in each population increasing over time, one where the populations are relatively constant over time, and a scenario where the parameter combination results in each population decreasing over time. Each of these three scenarios is paired with a temperature gradient that repeats as well as one that increasing roughly 5 degrees Celsius over the duration of the 20 year simulation. We therefore simulate a total of 6 scenarios. **show line plot of population for each?**


<font size="5">*Simulating Bottom Trawl Survey and Population Indexing*</font>


-Describe post hoc sampling process and how data is used

After each simulation is complete, we mimic the bottom trawl survey by conducting stratified random sampling in each inhabited strata twice each year. We sample in the same weeks that the Spring and Fall surveys take place and the number of the samples taken in each strata reflect true values. Most strata contain enough cells to sample a unique location in each survey over the duration of the simulation. For smaller strata we must repeat some sample locations. We then use the biomass collected from our samples in contemporary population indexing methods to estimate population trends. Knowing the true population values in our simulations allows us to compare the error calculated from each estimation method.


--Stratified mean vs VAST with and without covariates

We compare the yearly estimated of abundance obtained from the stratified mean to estimates obtained from the Vector-Autoregressive Spatio-temporal (VAST) model. The stratified mean is a typical survey-based approach that scales individual samples to the  strata-level by considering the area of each strata, before scaling to the region-level based on the relative size of each strata. VAST is a spatio-temporal statistical framework that models both abundance (biomass) and probability of occurrence (presence/absence). If desired, VAST also allows users to include covariate data to better inform the model. Covariates can be static (eg. habitat preferences) or dynamics (eg. temperature). The stratified mean calculations are straightforward and quick, while VAST models require numerous user inputs and take on the order of hours to complete.


We follow the advice given in **cite Thorson 2019 Guidance...** to build VAST models to estimate biomass on George's Bank using stratified mean samples from our model output. In addition to exploring different link functions and assumed distributions, our VAST model-building process included testing the impact of including spatial and/or spatio-temporal variation in our models, considering varying number of knots in our mesh, and testing different forms of temporal correlation. We also carried out the same process both including covariates in our model as well as running models without covariate information. We considered covaraites in the form of dyamic temperature values and/or static habitat values from our population model. When using covariates we ultimately decided to provide the most information to the model by including both covariates for both linear predictors. Since we know the true population values in our models we calculate the absolute error of each VAST estimate to compare between potential settings. Through this process, and in consultation with the VAST package creator, we determined setting that allowed VAST models to converge for all of our scenarios while also providing the lowest absolute error values. Settings for our VAST models can be seen in Table ???.

Our goal is to determine indexing approaches and settings that are robust to future environmental conditions and resulting spatial biomass patterns. An underlying assumption in all indexing methods that individual random samples combine to accurately represent true abudance by a) sampling all strata in which the population exists and b) low enough noise level in the samples to allow for a discernable pattern. This assumption can be questioned given enough noise in the sampling process **cite?** and/or shifting spatial preferences driven by climate change causing a population to move into a previously uninhabited strata. To simulate the impact of noise, indexing estimates after adding noise to our samples versus those using the true sampling values. To evaluate the effect of populations moving into new habitat, we compare indexing estimates using samples from all strata versus those that only include a subset of the full spatial domain for each species.

When combining population trends for each species, differing temperature scenarios, and sampling possibilities (noise, strata, covariates) there are a number of scenario combinations to consider. The columnbs in Table COMBOS show the possible choices that define each scenario.



<div align="center">Table COMBOS Each index estimate chooses one condition from each of the following columns. There are $3*3*2*2*2*2=144$ VAST model combinations and $3*3*2*2*2=72$ stratified mean estimates.</div>

```{r scenarios, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
library("knitr")
library("kableExtra")
library("dplyr")
library("readr")
library("tidyr")
library("forcats")


tabl <- "
|	Species	|	Population Trend | Temperature Scenario | Strata Included | Noise in Data | Covariates (VAST only) 
|:--	|:-- |:-- |:-- |:-- |:--- 
|Yellowtail | Increasing | Repeating | All strata | No Noise| No Covariates
|Cod | Constant | Increasing 5$^{\\circ}$| Subset | Yes Noise| Temp + Habitat
|Haddock | Decreasing | | | |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion


```


\section{Results}




```{r}


knitr::kable(YT_results, caption = "Yellowtail error results")

knitr::kable(Cod_results, caption = "Cod error results")

knitr::kable(Had_results, caption = "Haddock error results")
```


## Discussion 

A range of data-limited methods for setting catch advice were evaluated for stocks where assessment models may be rejected due to strong, positive retrospective patterns. A method was considered to perform well if it limited overfishing without resulting in light exploitation rates ($F << F_{MSY}$), thereby allowing depleted stocks to recover to $SSB_{MSY}$ (or for healthy stocks to remain there), and for high and stable catches (close to $MSY$).

Overall, none of the methods evaluated performed best across the scenarios exploring the different sources of the retrospective pattern (unreported catch or increasing $M$) and different levels of historical fishing intensity. A number of methods did perform well in many cases, however, while others performed consistently poorly, resulting in frequent and intense overfishing ($F>>F_{MSY}$). We performed simulations for a couple of scenarios with no source of retrospective patterns and found the expected result that all DLMs and the SCAA performed better ($SSB$, $F$, and catch were all closer to the $MSY$ reference points) than when either source of retrospective patterns was present. Due to the focus of this study, we did not examine the no retrospective source in detail and do not comment on it further.

Currently, in the Northeast U.S., if an assessment model is rejected due to a large rho value in $SSB$, the catch advice from that model is ignored and some data-limited approach is used. However, the rho-adjusted SCAA model performed better than a number of the alternatives explored here. Therefore, there should not necessarily be an expectation that a data-limited method will perform better than the rejected assessment model. The SCAA only resulted in high exploitation rates ($F >> F_{MSY}$) when unreported catch was the source of the retrospective pattern and for the scenario where $F=F_{MSY}$ at the end of the base period that left the stock in relatively good condition ($SSB \sim SSB_{MSY}$). In contrast, this method was particularly effective when the stock was depleted and there was unreported catch. When $M$ was the source of the retrospective pattern, the rho-adjusted SCAA method typically resulted in light exploitation rates, on average. The light exploitation rates in these cases were likely driven by the combination of using a rho-adjustment, but also using the lower $M$ from the beginning of the base period rather than the higher $M$ that occurred during the feedback period. Using an $M$ value that is too low in a stock assessment will typically bias estimates of biomass and reference points too low, resulting in catch advice that is below target levels [@Johnsonetal2014; @Puntetal2021M]. The consequences of using a value for $M$ that is too low versus too high is also asymmetrical [@Johnsonetal2014], with negative consequences being more severe when $M$ is assumed too high than low, and the results here are consistent with these previous conclusions.

The methods that adjusted recent average catches based on trends in the survey (Ismooth and Islope) performed well overall in terms of catch, stock status, and variation in catch. The method using the expanded survey biomass with the recent exploitation rate (ES-Frecent) also performed well and similarly to Ismooth. The performance of these methods was also generally robust among scenarios, with the exception of when there were unreported catches and the stock was depleted (see below). The generally positive performance of these methods was consistent with @Hilbornetal2002 and @CoxKronlund2008, both of which evaluated a variant of a "hold-steady" DLM. In the case of @Hilbornetal2002, the "hold-steady" DLM policy was designed to adjust catches in order to keep rockfish (*Sebastes spp.*) populations at recently observed index levels, and did so by functioning as a constant escapement harvest control rule where target catches were set to zero below some pre-specified index level. In the variant used by @CoxKronlund2008, catches were adjusted to maintain a sablefish (*Anoplopoma fimbria*) population at a pre-specified index level thought to be sustainable and desirable in terms of meeting fishery objectives (e.g., high catch), but never permitted target catches of zero and so functioned as a constant exploitation rate control rule. The "hold-steady" DLM of @CoxKronlund2008 performed similarly in terms of catch, stock depletion, and variation in catch, as a constant exploitation rate policy where target catch was specified as the product of desired exploitation rate and an estimate of biomass from a SCAA model. This result was robust to uncertainty in initial stock status and steepness [@CoxKronlund2008]. The SCAA model was always correctly specified (i.e., expected to produce unbiased estimates on average), however, and no comparison to the results of this research in the presence of retrospective patterns is possible [@CoxKronlund2008]. The "hold-steady" policy of @Hilbornetal2002 performed similarly to or better in terms of catch and stock status than other harvest control rules that relied on assessment estimates of biomass (i.e., 40:10 and constant *F*). The performance of the "hold-steady" DLM was also more robust to uncertainty in steepness and to the presence of unreported catch [@Hilbornetal2002]. The performance of the two harvest policies that relied on assessment estimates of biomass (i.e., constant exploitation rate and a "40:10" biomass-based policy) also degraded when the estimates of biomass were biased, which is an issue that does not effect the "hold-steady" DLM [@Hilbornetal2002]. The bias in the assessment estimates considered in @Hilbornetal2002 were not necessarily induced by a retrospective pattern, however, and no consideration of making a rho-adjustment was possible in that study. 

The Ismooth method is currently used to set catches for Georges Bank cod [@nefsc19] and red hake (*Urophycis chuss*; @nefsc20). Variations of the ES-Frecent have been used for witch flounder and Georges Bank yellowtail flounder. While the findings here generally support the continued use of the Ismooth and ES-Frecent methods, they may not be well suited for depleted stocks where unreported catches are believed to be an issue. The Ismooth, Islope, and ES-Frecent DLMs produced high $Fs$ and limited stock recovery with unreported catches and when the stock was depleted. While @Hilbornetal2002 and @CoxKronlund2008 did not reach the same conclusion about the "hold-steady" DLM, those studies did not consider initial levels of depletion as low as in this study. These results highlight the importance of accurate catch reporting, as unreported catch can create a negative feedback loop with perpetually high $Fs$ being produced by a management system that seemingly should result in sustainable catch advice.

Three methods were consistently risk-averse across scenarios, limiting the frequency and magnitude of overfishing and resulting in high stock biomass. These methods were the two catch curve options (CC-FM and CC-FSPR) and DynLin. The catch curve methods produced a wider range of average catches across scenarios, and also had greater interannual variability in catches compared to DynLin. While the lower exploitation rates from these approaches may be undesirable due to forgone yield, there may be circumstances where they are preferred. For example, for stocks that are believed to be heavily depleted, low exploitation rates would allow for a more rapid recovery. 

A number of methods performed poorly, particularly when catches were unreported. These methods include three of the expanded survey biomass approaches (ES-Fstable, ES-FM, ES-FSPR), AIM, and Skate. The AIM model has been widely used across stocks in the region [@nefsc02a; @nefsc05; @nefsc08], although there is a decreasing trend in its use across model resistant stocks [@nefsc19]. The findings here suggest that alternative approaches should be considered in cases where AIM is still used and there is concern over unreported catches. The Skate method is used to manage the skate complex in the Northeast U.S. (a group of seven co-managed species). Interestingly, six of the seven species are considered in good condition with high survey biomass indices in recent years [@nefmc20]. That the Skate method performed poorly in our analysis but performs well for the skate complex illustrates how the performance of methods in this analysis may be sensitive to the scenarios and species life history considered. As may be the case for the Skate method, the performance of some methods may depend on the condition of the stock when the method is first applied, and less so on life-history. Therefore, care is needed when trying to generalize these results across stocks that may have different life histories, exploitation histories, and without unreported catches or increases in $M$. 

In addition to the analytical differences among the thirteen DLMs, most of the DLMs and control rules had multiple options that could be adjusted to make them more or less risk averse. DynLin had a large number of user defined decision points. Given the large range of options already explored in the study, one suite of options was selected for each DLM-control rule and kept constant for all simulations. Further studies could explore the different options within an individual DLM to understand how they might affect performance. 

Many other data-limited methods exist for setting catch advice that were not included in this evaluation, and they vary widely in complexity, data inputs, and assumptions required [e.g., @carruthers2018dlm]. Length based methods were not evaluated to keep the overall number of methods tractable, and due to the availability of age based information in the region. Methods that require only catch data or snap shots of survey data were not considered due to the availability of the relatively long and contiguous Northeast Fisheries Science Center's spring and fall, coastwide bottom trawl surveys, and the fact that "catch only" methods have been shown to perform poorly [e.g., @carruthers2014eval]. Complete catch histories are not available for stocks in the region (i.e., from the inception of fishing). Consequently, methods that required complete catch histories or required assumptions about relative depletion [e.g., DCAC in @maccall2009dca; DB-SRA in @dick2022dsra] were also omitted from consideration. The need for short run-times and the desire for methods that could be reviewed quickly prevented the use of modern state-space production models such as SPiCT [@pedersen2017spict] and JABBA [@winker2018jabba].

The SCAA was confronted with inconsistent data in this study, while the DLMs typically used only a single source of data and thus did not encounter inconsistencies. A recent examination of the data used in assessments in this region similarly found inconsistencies in data streams even before modeling. @wiedenmann2022strange found a negative relationship between relative F (catch/survey) and survey Z for stocks with strong retrospective patterns but the expected positive relationship for stocks without a retrospective pattern. It is exactly this sort of tension that creates retrospective patterns in integrated models, but is not found in DLMs that only use one type of data.

Despite conducting hundreds of thousands of simulations, there are still limitations to our study. We only examined one life history representative of groundfish in the region. We acknowledge that best practice is to select a DLM for a specific life history and fishery condition [e.g., @fischer2020dlm]. As is typically the case with large simulation studies, we were not able to tune any of the DLMs or the SCAA in any given realization, which would occur in practice for an actual stock assessment. We also examined only scenarios that started with Mohn's rho values near 0.5 for spawning stock biomass. This is a strong retrospective pattern, but some stocks in the region have even stronger retrospectives. Performance of the DLMs and SCAA would be expected to degrade with stronger retrospectives, but by how much is still an open area for research. Similarly, sources of retrospective patterns that create different relationships between the true values and estimated values should also be explored [see @deroba2014retro]. To make the results interpretable, we only examined a single source for the retrospective pattern at a time. In reality, there may be more than one factor leading to an observed retrospective pattern. How the multiple sources would interact to influence performance is another topic for future research. Development of harvest control rules specifically for situations where retrospective patterns are found in age-based assessments would also be beneficial. The large number of scenarios examined and the large number of realizations gives us confidence that our results are meaningful in general, but that the performance of any of the DLMs may differ in actual practice. 

An interesting finding of this study is the linear versus diffuse patterns between $SSB$ and catch across methods. These patterns have implications for the trade-offs among methods, with linear relationships resulting in more consistent exploitation rates across stock sizes. Therefore, these methods have higher certainty of a given catch at a given stock size. However, they also tended to result in lower stock sizes, on average, across methods. The more diffuse relationships resulted in more variable exploitation rates across stock sizes, with some situations where the population biomass was quite high but the catch was low (relative to MSY), resulting in a very low $F$. The reasons behind these different patterns remain unclear, and future work to explore these patterns is warranted. 

One of the reasons for the difference in performance between the catch and natural mortality retrospective sources was how the reference points were calculated. In all cases, the initial conditions, including the natural mortality rate, were used to compute the reference points. This decision was made based on the fact that the increase in natural mortality was assumed to be unknown in the simulations. If the increase in natural mortality was known, the age-structured assessments would have accounted for it, different reference points might have been computed [@legault2016increaseM] and there may not have been a retrospective pattern at all [@legault2020rose], and no need to consider alternative DLMs. The reference points for the increased $M$ scenarios would have been different if they were computed using the values from the final year of the base period, but the overall conclusions regarding the different DLMs would not change as this just results in a rescaling of the axis. These results are not shown to reduce confusion regarding the simulations.

Closed-loop simulation is a common tool for examining performance of catch advice from various stock assessment approaches in a feedback setting. It is often used as part of a full management strategy evaluation when working with stakeholders to develop management regulations that make trade offs between near term and long term catches, risk to the fish population, and mixed-fleet allocations [@carruthers2016simpleMPs; @goethel2019mse; @harlyan2019hcr]. We did not conduct a full management strategy evaluation with stakeholder input [@goethel2019stakeholder], but see that as a fruitful next step that could build on the conclusions from our closed-loop work. Using a generic groundfish life-history and monitoring standard performance metrics related to stock status and catch stability, we were able to cull the herd of potential DLMs and we would not carry the consistent poor performers forward for further study.  The wide range of expertise reflected in the authorship was by design so that the simulation specifications and performance metrics were broadly useful. Before undertaking a full management strategy evaluation and engaging regional stakeholders, we would want to select a specific stock and jointly identify specific management regulations to be tested [@deroba2019dream]. Results of this work have been presented to both local fishery management councils, with generally positive feedback about the utility of the conclusions for identifying appropriate model approaches when an SCAA is rejected. Our work was similar to all other closed-loop simulations in that it was designed to address a specific situation, including much recent work comparing the performance of data-limited and data rich assessment approaches [e.g., @fulton2016datarich; @sagarese2019dlm; @bouch2020datapoor; @li2022dlm].

This study is a first attempt to identify suitable methods for setting catch advice when stock assessment models are rejected due to large, positive retrospective patterns. Although no single method performed best across scenarios, a number of generally suitable and unsuitable methods were identified under specific conditions. The results of this work can help scientists and managers select a subset of possible options for consideration to set catch advice when assessment models are rejected. The approach developed here can, and should be expanded to consider other cases not explored here, as performance of individual methods are very likely case-dependent. 

## Acknowledgements
We thank the Index-Based Methods and Control Rules Research Track review panel of Paul Rago (chair), Yong Chen, Robin Cook, and Paul Medley for feedback on preliminary results, three anonymous reviewers and the associate editor for reviewing an earlier version of this work. The scientific results and conclusions, as well as any views or opinions expressed herein, are those of the authors and do not necessarily reflect those of NOAA or the Department of Commerce.   

## Data and Code Availability
All data and code used in this work are available at https://github.com/cmlegault/IBMWG.

## References

<div id="refs"></div>



\pagebreak

## Tables
</font>

\pagebreak

<div align="center">Table 1. Parameters used in all population models.</div>

```{r paramsALL, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
library("knitr")
library("kableExtra")
library("dplyr")
library("readr")
library("tidyr")
library("forcats")


tabl <- "
|	Parameter	|	Description | Unit |Yellowtail | Cod | Haddock | Source
|:---	|:------- |:-- |:--- |:-- |:-- |:--
| $\\rho$ | Ford's growth coefficient | wk$^{-1}$  | 4.48 | 4.43 | 4.49 | |
| $M$ | Natural Mortality| wk$^{-1}$  | 0.2064 | 0.2728 | 0.3340 | |
| $W_R$ |Weight of fully recruited fish| kg | 0.39 | 2.95 | 1.12 | |
| $W_{R-1}$ | Weight of pre-recruit fish| kg | 0.13 | 0.39 | 0.19 | |
| $\\sigma^2$ | Variance in recruited fish | kg$^2$ | 0.55 | 0.55 | 0.55 ||
| $\\lambda$ | Decay rate for movement | - | 0.7 | 0.7 | 0.7 ||
| $Spwn_s$ | Spawning weeks for species $s$ | wk | 9-12 | 8-13 | 11-14 ||
| $Rec_s$ | Recruitment weeks for species $s$ | wk | 9-12 | 8-13 | 11-14 ||
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion


```


```{r paramsALL2,echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
library("knitr")
library("kableExtra")
library("dplyr")
library("readr")
library("tidyr")
library("forcats")



param_table1 = read_csv("Parameters1.csv")

summary_tab1 <- param_table1 %>% 
 # dplyr::mutate_if(is.numeric, list(~as.character(signif(., 3))))%>%
  
  mutate_if(grepl('rrr',.), ~replace(., grepl('rrr', .), paste("\U03C1")))#should be replacing r with the rho symbol
 # mutate_if(grepl('W1',.), ~replace(., grepl('W1', .), "WW")) #should be replacing beta with the symbol
 #    mutate_if(grepl('wk',.), ~replace(., grepl('wk', .), "1/wk")) %>% #should be replacing beta with the symbol
 #  

# Change the orientation of the table so that Species become the variable headings



# Create table to present the data]
Fig_1 <- kable(summary_tab1, 
  caption = "Parameters used in all population models.",
    format = "latex", booktabs = TRUE) %>%
  kable_styling(font_size = 10)
 
```


 ```{r, results='asis'}
 
 Fig_1
 
 ```


<!-- Table 2. Parameters used in models with relatively constant populations. -->
<!-- ```{r paramsCON, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'} -->
<!-- tabl <- " -->
<!-- |		|	Description | Yellowtail | Cod | Haddock  -->
<!-- |:---	|:------- |:--- |:-- |:-- |:-- -->
<!-- | $M+F$ | Adjusted Mortality (Natural + Fishing)| wk$^{-1}$  | 0.764 | 0.83 | 0.309 |  -->

<!-- " -->
<!-- cat(tabl) # output the table in a format good for HTML/PDF/docx conversion -->
<!-- ``` -->

<!-- Table 3. Parameters used in models with increasing populations. -->
<!-- ```{r paramsINC, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'} -->
<!-- tabl <- " -->
<!-- |		|	Description | Yellowtail | Cod | Haddock | Source -->
<!-- |:---	|:------- |:--- |:-- |:-- |:-- -->
<!-- | $M+F$ | Adjusted Mortality (Natural + Fishing)| wk$^{-1}$  | 0.564 | 0.372 |  0.134 |  -->

<!-- " -->
<!-- cat(tabl) # output the table in a format good for HTML/PDF/docx conversion -->
<!-- ``` -->


<!-- Table 4. Parameters used in models with decreasing populations. -->
<!-- ```{r paramsDEC, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'} -->
<!-- tabl <- " -->
<!-- |		|	Description | Yellowtail | Cod | Haddock | Source -->
<!-- |:---	|:------- |:--- |:-- |:-- |:-- -->
<!-- | $M+F$ | Adjusted Mortality (Natural + Fishing)| wk$^{-1}$  | 0.764 | 0.623 | 0.334 |  -->

<!-- " -->
<!-- cat(tabl) # output the table in a format good for HTML/PDF/docx conversion -->
<!-- ``` -->


<!-- DOESNT WORK!!
<!-- Table 2. Parameters used in population models for each scenario. -->
<!-- ```{r paramsSCENARIOS, echo=FALSE, message=TRUE, warnings=TRUE, results='asis'} -->
<!-- library(huxtable) -->

<!-- ht <- tribble_hux( -->
<!--   ~ "Section/topic", ~ "Item no", ~ "Checklist item", ~ "Reported on Page No", -->
<!--   "Title"          , ""         , ""                , "", -->
<!--   "Title"          , "1"        , "Identify..."     , "",  -->
<!--   "Abstract"       , ""         , ""                , "", -->
<!--   "Structured summary", "2"     , "Provide..."      , "" -->
<!--   # et cetera... -->
<!-- )  -->


<!-- # using the pipe from 4.1.0... -->

<!-- ht |>  -->
<!--   set_header_rows(c(2, 4), TRUE)                |> -->
<!--   merge_across(c(2, 4), everywhere)             |> -->
<!--   style_header_rows(bold = TRUE)                |> -->
<!--   set_all_borders(brdr(0.4, "solid", "grey70")) |> -->
<!--   set_background_color("grey97")                |> -->
<!--   set_background_color(1, 1:3, "grey90")        |> -->
<!--   set_col_width(c(0.2, 0.05, 0.55, 0.2))        |> -->
<!--   set_font("cmss")                               -->
<!-- ``` -->


<!-- ```{r,echo=FALSE, message=FALSE, warnings=FALSE, results='asis'} -->
<!-- library("knitr") -->
<!-- library("kableExtra") -->
<!-- library("dplyr") -->
<!-- library("readr") -->
<!-- library("tidyr") -->
<!-- library("forcats") -->

<!-- #orig_ui_deaths = read_csv("https://www.opendata.nhs.scot/dataset/b0135993-3d8a-4f3b-afcf-e01f4d52137c/resource/89807e07-fc5f-4b5e-a077-e4cf59491139/download/ui_deaths_2020.csv") -->

<!-- summary_tab <- orig_ui_deaths %>% -->
<!--   # Apply filters in same was as in plots so looking at one year, whole of Scotland, and exlucing "All" entries -->
<!--   filter(Year == "2018", HBR == "S92000003", AgeGroup != "All" & Sex != "All", InjuryType != "Accidental exposure" & InjuryType != "All") %>% -->
<!-- # Group the table according to injury type, age, and sex -->
<!--   group_by(InjuryType, AgeGroup, Sex) %>% -->
<!-- # Create a summary of total number of deaths for neater appearance to the table and demonstration of the figures -->
<!--   summarise(total_deaths = sum(NumberofDeaths)) %>% -->
<!-- # Change the orientation of the table so that age groups become the variable headings -->
<!--   pivot_wider(names_from = AgeGroup, values_from = total_deaths) %>% -->
<!--   mutate(Sex = factor(Sex)) %>% -->
<!--   arrange(Sex) %>% -->
<!--   select(Sex, everything()) -->

<!-- # Create table to present the data] -->
<!-- Fig_2 <- kable(summary_tab[, -1],  -->
<!--   caption = "Deaths from unintentional injuries in Scotland", -->
<!--     format = "latex", booktabs = TRUE) %>% -->
<!--   kable_styling(font_size = 10) %>% -->
<!--   pack_rows(tab_kable, colnum = 1, -->
<!--     index = table(fct_inorder(summary_tab$Sex), useNA = "no")) -->
<!-- ``` -->
<!-- ```{r, results='asis'} -->
<!-- Fig_2 -->
<!-- ``` -->



```{r paramsSCENARIOS,echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
library("knitr")
library("kableExtra")
library("dplyr")
library("readr")
library("tidyr")
library("forcats")



param_table = read_csv("Parameters.csv")

summary_tab <- param_table %>% 
  dplyr::mutate_if(is.numeric, list(~as.character(signif(., 3))))%>%
  
 mutate_if(grepl('alpha',.), ~replace(., grepl('alpha', .), paste("\U03B1"))) %>% #should be replacing alpha with the symbol
  mutate_if(grepl('beta',.), ~replace(., grepl('beta', .), paste("\U03B2"))) %>% #should be replacing beta with the symbol
    mutate_if(grepl('wk',.), ~replace(., grepl('wk', .), "1/wk")) %>% #should be replacing beta with the symbol
  
# Group the table according to Parameter, Scenario
  group_by(Parameter, Scenario)%>%
# Change the orientation of the table so that Species become the variable headings
  pivot_wider(names_from = Species,  values_from = Value) %>%
  mutate(Scenario = factor(Scenario)) %>%
  arrange(Scenario) %>%
  select(Scenario, everything())



# Create table to present the data]
Fig_3 <- kable(summary_tab[, -1], 
  caption = "Parameters used in population models for each scenario.",
    format = "latex", booktabs = TRUE) %>%
  kable_styling(font_size = 10) %>%
  pack_rows(tab_kable, colnum = 1,
    index = table(fct_inorder(summary_tab$Scenario), useNA = "no"))
```
```{r, results='asis'}
Fig_3
```





<div align="center">Table XX. Parameters used in all VAST models.</div>

```{r VASTsettings, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}


tabl <- "
|	Parameter	|	Description | Input/Value
|:-	|:------- |:----
| *ObsModel* | Link function and assumed distribution | c(10,2)
|FieldCOnfig | Specified spatial and/or spatio-temporal variation in predictors| c(Omega1=0, Epsilon1=0, Omega2=1, Epsilon2=1)
| RhoConfig | Specifying whether intercepts or spatio-temporal variation is structured among time intervals | c(Beta1=3, Beta2=3, Epsilon1=0, Epsilon2=4)
| X1_formula | Right-sided formula affecting the 1st linear predictor  | X1_formula = ~ poly(Temp, degree=2 )
| X2_formula | Right-sided formula affecting the 2nd linear predictor | X2_formula = ~ poly(Temp, degree=2 ) + poly(Habitat, degree=2 )


"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion


```








<!-- ```{r PMsallscenarios, echo=FALSE, message=FALSE, warnings=FALSE, out.width='7in', fig.cap=figcap1} -->
<!-- #knitr::include_graphics(file.path(getwd(),"MS_tables_figs_rev1/final_ms_figs","Figure_1.pdf")) -->
<!-- ``` -->


<!-- ```{r SSmsyvsCMSY, echo=FALSE, message=FALSE, warnings=FALSE, out.width='7in', fig.cap=figcap2} -->
<!-- #knitr::include_graphics(file.path(getwd(),"MS_tables_figs_rev1/final_ms_figs","Figure_2.pdf")) -->
<!-- ``` -->


<!-- ```{r PMsbyretroF, echo=FALSE, message=FALSE, warnings=FALSE, out.width='6in', fig.cap=figcap3} -->
<!-- #knitr::include_graphics(file.path(getwd(),"MS_tables_figs_rev1/final_ms_figs","Figure_3.pdf")) -->
<!-- ``` -->


<!-- ```{r FFmsybyscenario, echo=FALSE, message=FALSE, warnings=FALSE, out.width='6in', fig.cap=figcap4} -->
<!-- #knitr::include_graphics(file.path(getwd(),"MS_tables_figs_rev1/final_ms_figs","Figure_4.pdf")) -->
<!-- ``` -->


<!-- ```{r linearvsdiffuse, echo=FALSE, message=FALSE, warnings=FALSE, out.width='7in', fig.cap=figcap5} -->
<!-- #knitr::include_graphics(file.path(getwd(),"MS_tables_figs_rev1/final_ms_figs","Figure_5.pdf")) -->
<!-- ``` -->
